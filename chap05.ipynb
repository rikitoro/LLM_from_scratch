{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e9ef54",
   "metadata": {},
   "source": [
    "# 5. ラベルなしでの事前学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78778679",
   "metadata": {},
   "source": [
    "## 5.1 生成テキストモデルを評価する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c387c06",
   "metadata": {},
   "source": [
    "### GPTを使ってテキストを生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2813e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPTModel import GPTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from GPTModel import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "  \"vocab_size\": 50257,\n",
    "  \"context_length\": 256,\n",
    "  \"emb_dim\": 768,\n",
    "  \"n_heads\": 12,\n",
    "  \"n_layers\": 12,\n",
    "  \"drop_rate\": 0.1,\n",
    "  \"qkv_bias\": False,  \n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1353b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 5-1\n",
    "\n",
    "import tiktoken\n",
    "from util import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dim\n",
    "  return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "  flat = token_ids.squeeze(0) # remove batch dim.\n",
    "  return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "  model=model,\n",
    "  idx=text_to_token_ids(start_context, tokenizer),\n",
    "  max_new_tokens=10,\n",
    "  context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d192e",
   "metadata": {},
   "source": [
    "### テキスト生成の損失を計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da1a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,     1107, 588]])  #  \"I really like\"]\n",
    "targets = torch.tensor ([[3626, 6100, 345],   # [\" effort moves you\",\n",
    "                         [1107, 588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8347e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim= -1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc08e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c41f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idx = 0\n",
    "target_brabs_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_brabs_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_brabs_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_brabs_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probas = torch.log(torch.cat((target_brabs_1, target_brabs_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12885e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89322ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87060b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82809d",
   "metadata": {},
   "source": [
    "### 訓練データセットと検証データセットで損失を計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507327b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "  text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx   = int(train_ratio * len(text_data))\n",
    "train_data  = text_data[:split_idx]\n",
    "val_data    = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddfd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import create_dataloader_v1\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "  train_data,\n",
    "  batch_size=2,\n",
    "  max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "  stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "  drop_last=True,\n",
    "  shuffle=True,\n",
    "  num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "  val_data,\n",
    "  batch_size=2,\n",
    "  max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "  stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "  drop_last=False,\n",
    "  shuffle=False,\n",
    "  num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eece9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "  print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "  print(x.shape, y.shape)\n",
    "  # print(x)\n",
    "  # print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "  input_batch = input_batch.to(device)\n",
    "  target_batch = target_batch.to(device)\n",
    "  logits = model(input_batch)\n",
    "  loss = torch.nn.functional.cross_entropy(\n",
    "    logits.flatten(0, 1), target_batch.flatten()\n",
    "  )\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 5-2\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "  total_loss = 0.\n",
    "  if len(data_loader) == 0:\n",
    "    return float(\"nan\")\n",
    "  elif num_batches is None:\n",
    "    num_batches = len(data_loader)\n",
    "  else:\n",
    "    num_batches = min(num_batches, len(data_loader))\n",
    "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "    if i < num_batches:\n",
    "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "      total_loss += loss.item()\n",
    "    else:\n",
    "      break\n",
    "  return total_loss/num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a77cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  train_loss  = calc_loss_loader(train_loader, model, device)\n",
    "  val_loss    = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss    :\", train_loss)\n",
    "print(\"Validataion loss :\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f0798",
   "metadata": {},
   "source": [
    "## 5.2 LLMを訓練する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    train_loss  = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "    val_loss    = calc_loss_loader(val_loader,   model, device, num_batches=eval_iter)\n",
    "\n",
    "  model.train()\n",
    "  return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "  model.eval()\n",
    "  context_size = model.pos_emb.weight.shape[0]\n",
    "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "  with torch.no_grad():\n",
    "    token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size=context_size)\n",
    "\n",
    "  decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "  print(decoded_text.replace(\"\\n\", \" \"))\n",
    "  model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 5-3\n",
    "\n",
    "def train_model_simple( model, train_loader, val_loader, optimizer, device,\n",
    "                        num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "  tokens_seen, global_step = 0, -1\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for input_batch, target_batch in train_loader:\n",
    "      optimizer.zero_grad()\n",
    "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      tokens_seen += input_batch.numel()\n",
    "      global_step += 1\n",
    "\n",
    "      if global_step % eval_freq == 0:\n",
    "        train_loss, val_loss = evaluate_model( model, train_loader, val_loader, device, eval_iter)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        track_tokens_seen.append(tokens_seen)\n",
    "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "              f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        \n",
    "    generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "  return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "  model, train_loader, val_loader, optimizer, device,\n",
    "  num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "  start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epoches_seen, tokens_seen, trains_losses, val_losses):\n",
    "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "  ax1.plot(epoches_seen, train_losses, label=\"Training loss\")\n",
    "  ax1.plot(epoches_seen, val_losses, linestyle=\"-.\", label=\"Validataion loss\")\n",
    "  ax1.set_xlabel(\"Epochs\")\n",
    "  ax1.set_ylabel(\"Loss\")\n",
    "  ax1.legend(loc=\"upper right\")\n",
    "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "  ax2 = ax1.twiny()\n",
    "  ax2.plot(tokens_seen, trains_losses, alpha=0)\n",
    "  ax2.set_xlabel(\"Tokens seen\")\n",
    "  fig.tight_layout()\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01458b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6efcc2",
   "metadata": {},
   "source": [
    "## 5.3 ランダム性をコントロールするデコーディング戦略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb591c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
